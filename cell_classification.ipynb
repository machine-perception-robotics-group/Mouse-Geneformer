{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234afff3",
   "metadata": {},
   "source": [
    "## Geneformer Fine-Tuning for Cell Annotation Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cbe6178-ea4d-478a-80a8-65ffaa4c1820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GPU_NUMBER = [0]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(s) for s in GPU_NUMBER])\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9885d9f-00ac-4c84-b6a3-b7b648a90f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import pickle\n",
    "import subprocess\n",
    "import seaborn as sns; sns.set()\n",
    "from datasets import load_from_disk\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "from transformers import Trainer\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "from geneformer import DataCollatorForCellClassification\n",
    "import sys\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd3b98-5409-4105-b7af-f1ff64ea6a72",
   "metadata": {},
   "source": [
    "## Prepare training and evaluation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735f1b7-7595-4a02-be17-2c5b970ad81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cell type or disease dataset (includes all tissues)\n",
    "\n",
    "\n",
    "# select fine turning type (ctc: cell type classification or isp: in silico perturbation)\n",
    "f_type = \"isp\"\n",
    "\n",
    "# dataset_name(xxx.dataset path)\n",
    "dataset_name = \"/path/to/your/dataset/to/analysis/xxx.dataset/\"\n",
    "\n",
    "# load dataset\n",
    "train_dataset = load_from_disk(dataset_name)\n",
    "\n",
    "# check and remove column names\n",
    "if f_type == \"isp\" :\n",
    "    try :\n",
    "        print(np.unique(train_dataset[\"disease\"]))\n",
    "    except KeyError as e :\n",
    "        print(\"KeyError: {}\".format(e))\n",
    "        print(\"changing to disease\")\n",
    "        train_dataset = train_dataset.rename_column(\"column name in diseases infomation\",\"disease\")\n",
    "        print(\"change finished\")\n",
    "        print(np.unique(train_dataset[\"disease\"]))\n",
    "    \n",
    "elif f_type == \"ctc\" :\n",
    "    try :\n",
    "        print(np.unique(train_dataset[\"cell_type\"]))\n",
    "    except KeyError as e :\n",
    "        print(\"KeyError: {}\".format(e))\n",
    "        print(\"changing to cell_type\")\n",
    "        train_dataset = train_dataset.rename_column(\"column name in cell types infomation\",\"cell_type\")\n",
    "        print(\"change finished\")\n",
    "        print(np.unique(train_dataset[\"cell_type\"]))\n",
    "\n",
    "else :\n",
    "    print(\"error: select fine turning type (ctc or isp)\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "print(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf4ca68-4a0d-4d6e-89a3-609acce6f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove cache files in xxx.dataset \n",
    " \n",
    "import glob\n",
    "import os\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "rmfiles = glob.glob(dataset_name+\"/cache*\")\n",
    "\n",
    "if rmfiles == [] :\n",
    "    print(\"not exist cache files\")\n",
    "else :\n",
    "    for tqdm_i2, rmfile in zip(tqdm(rmfiles, desc='remove files loop'), rmfiles) :\n",
    "        os.remove(rmfile)\n",
    "    print(\"Finished removeing cache file in it !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9229668-4239-4de7-8f68-7f3c3e46e089",
   "metadata": {},
   "source": [
    "## Cell Type Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4297a02-4c4c-434c-ae55-3387a0b239b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you do cell type classification task, you have to run this cell. Otherwise, if you do disease type classification (in silico perturbation) task, you have to run the cell below.\n",
    "\n",
    "dataset_list = []\n",
    "evalset_list = []\n",
    "organ_list = []\n",
    "target_dict_list = []\n",
    "\n",
    "\n",
    "for organ in Counter(train_dataset[\"organ_major\"]).keys():\n",
    "    # collect list of tissues for fine-tuning (immune and bone marrow are included together)\n",
    "    if organ in [\"bone_marrow\"]:  \n",
    "        continue\n",
    "    elif organ==\"immune\":\n",
    "        organ_ids = [\"immune\",\"bone_marrow\"]\n",
    "        organ_list += [\"immune\"]\n",
    "    else:\n",
    "        organ_ids = [organ]\n",
    "        organ_list += [organ]\n",
    "    \n",
    "    print(organ)\n",
    "    \n",
    "    # filter datasets for given organ\n",
    "    def if_organ(example):\n",
    "        return example[\"organ_major\"] in organ_ids\n",
    "    trainset_organ = train_dataset.filter(if_organ, num_proc=16)\n",
    "    \n",
    "    # per scDeepsort published method, drop cell types representing <0.5% of cells\n",
    "    celltype_counter = Counter(trainset_organ[\"cell_type\"])\n",
    "    total_cells = sum(celltype_counter.values())\n",
    "    cells_to_keep = [k for k,v in celltype_counter.items() if v>(0.005*total_cells)]\n",
    "    def if_not_rare_celltype(example):\n",
    "        return example[\"cell_type\"] in cells_to_keep\n",
    "    trainset_organ_subset = trainset_organ.filter(if_not_rare_celltype, num_proc=16)\n",
    "      \n",
    "    # shuffle datasets and rename columns\n",
    "    trainset_organ_shuffled = trainset_organ_subset.shuffle(seed=42)\n",
    "    trainset_organ_shuffled = trainset_organ_shuffled.rename_column(\"cell_type\",\"label\")\n",
    "    trainset_organ_shuffled = trainset_organ_shuffled.remove_columns(\"organ_major\")\n",
    "    \n",
    "    # create dictionary of cell types : label ids\n",
    "    target_names = list(Counter(trainset_organ_shuffled[\"label\"]).keys())\n",
    "    target_name_id_dict = dict(zip(target_names,[i for i in range(len(target_names))]))\n",
    "    target_dict_list += [target_name_id_dict]\n",
    "    \n",
    "    # change labels to numerical ids\n",
    "    def classes_to_ids(example):\n",
    "        example[\"label\"] = target_name_id_dict[example[\"label\"]]\n",
    "        return example\n",
    "    labeled_trainset = trainset_organ_shuffled.map(classes_to_ids, num_proc=16)\n",
    "    \n",
    "    # create 80/20 train/eval splits\n",
    "    labeled_train_split = labeled_trainset.select([i for i in range(0,round(len(labeled_trainset)*0.8))])\n",
    "    labeled_eval_split = labeled_trainset.select([i for i in range(round(len(labeled_trainset)*0.8),len(labeled_trainset))])\n",
    "    \n",
    "    # filter dataset for cell types in corresponding training set\n",
    "    trained_labels = list(Counter(labeled_train_split[\"label\"]).keys())\n",
    "    def if_trained_label(example):\n",
    "        return example[\"label\"] in trained_labels\n",
    "    labeled_eval_split_subset = labeled_eval_split.filter(if_trained_label, num_proc=16)\n",
    "\n",
    "    dataset_list += [labeled_train_split]\n",
    "    evalset_list += [labeled_eval_split_subset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928e4d33-4d25-4a60-bc2b-63fd2e56fbd5",
   "metadata": {},
   "source": [
    "## Disease Type Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a87aa2-c8eb-4cf2-8b1e-ec223b416de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disease classification\n",
    "\n",
    "dataset_list = []\n",
    "evalset_list = []\n",
    "organ_list = []\n",
    "target_dict_list = []\n",
    "\n",
    "\n",
    "for organ in Counter(train_dataset[\"organ_major\"]).keys():\n",
    "    # collect list of tissues for fine-tuning (immune and bone marrow are included together)\n",
    "    \n",
    "    organ_ids = [organ]\n",
    "    organ_list += [organ]\n",
    "    \n",
    "    print(organ)\n",
    "\n",
    "    # filter datasets for given organ\n",
    "    def if_organ(example):\n",
    "        return example[\"organ_major\"] in organ_ids\n",
    "    trainset_organ = train_dataset.filter(if_organ, num_proc=16)\n",
    "    \n",
    "    # per scDeepsort published method, drop cell types representing <0.5% of cells\n",
    "    celltype_counter = Counter(trainset_organ[\"disease\"])\n",
    "    total_cells = sum(celltype_counter.values())\n",
    "    cells_to_keep = [k for k,v in celltype_counter.items() if v>(0.005*total_cells)]\n",
    "    def if_not_rare_celltype(example):\n",
    "        return example[\"disease\"] in cells_to_keep\n",
    "    trainset_organ_subset = trainset_organ.filter(if_not_rare_celltype, num_proc=16)\n",
    "      \n",
    "    # shuffle datasets and rename columns\n",
    "    trainset_organ_shuffled = trainset_organ_subset.shuffle(seed=42)\n",
    "    trainset_organ_shuffled = trainset_organ_shuffled.rename_column(\"disease\",\"label\")\n",
    "    trainset_organ_shuffled = trainset_organ_shuffled.remove_columns(\"organ_major\")\n",
    "    \n",
    "    # create dictionary of cell types : label ids\n",
    "    target_names = list(Counter(trainset_organ_shuffled[\"label\"]).keys())\n",
    "    target_name_id_dict = dict(zip(target_names,[i for i in range(len(target_names))]))\n",
    "    target_dict_list += [target_name_id_dict]\n",
    "    \n",
    "    # change labels to numerical ids\n",
    "    def classes_to_ids(example):\n",
    "        example[\"label\"] = target_name_id_dict[example[\"label\"]]\n",
    "        return example\n",
    "    labeled_trainset = trainset_organ_shuffled.map(classes_to_ids, num_proc=16)\n",
    "    \n",
    "    # create 80/20 train/eval splits\n",
    "    labeled_train_split = labeled_trainset.select([i for i in range(0,round(len(labeled_trainset)*0.8))])\n",
    "    labeled_eval_split = labeled_trainset.select([i for i in range(round(len(labeled_trainset)*0.8),len(labeled_trainset))])\n",
    "    \n",
    "    # filter dataset for cell types in corresponding training set\n",
    "    trained_labels = list(Counter(labeled_train_split[\"label\"]).keys())\n",
    "    def if_trained_label(example):\n",
    "        return example[\"label\"] in trained_labels\n",
    "    labeled_eval_split_subset = labeled_eval_split.filter(if_trained_label, num_proc=16)\n",
    "\n",
    "    dataset_list += [labeled_train_split]\n",
    "    evalset_list += [labeled_eval_split_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d79e9-b6e2-42e0-8cd0-ec7415cbe1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell must run\n",
    "\n",
    "trainset_dict = dict(zip(organ_list,dataset_list))\n",
    "traintargetdict_dict = dict(zip(organ_list,target_dict_list))\n",
    "\n",
    "evalset_dict = dict(zip(organ_list,evalset_list))\n",
    "\n",
    "\n",
    "print(trainset_dict)\n",
    "print(traintargetdict_dict)\n",
    "\n",
    "print(evalset_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb110d-ba43-4efc-bc43-1815d6912647",
   "metadata": {},
   "source": [
    "## Fine-Tune With Cell Classification Learning Objective and Quantify Predictive Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd7b1cfb-f5cb-460e-ae77-769522ece054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)\n",
    "    pre = precision_score(labels, preds, average='macro')\n",
    "    rec = recall_score(labels, preds, average='macro')\n",
    "    macro_f1 = f1_score(labels, preds, average='macro')\n",
    "    return {\n",
    "      'accuracy': acc,\n",
    "      'macro_precision': pre,\n",
    "      'macro_recall': rec,\n",
    "      'macro_f1': macro_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaab7a4-cc13-4e8f-b137-ed18ff7b633c",
   "metadata": {},
   "source": [
    "### Please note that, as usual with deep learning models, we **highly** recommend tuning learning hyperparameters for all fine-tuning applications as this can significantly improve model performance. Example hyperparameters are defined below, but please see the \"hyperparam_optimiz_for_disease_classifier\" script for an example of how to tune hyperparameters for downstream applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24e1ab7-0131-44bd-b458-1ce5ba31853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model parameters\n",
    "# max input size\n",
    "max_input_size = 2**11  # 2048\n",
    "\n",
    "# set training hyperparameters\n",
    "# max learning rate\n",
    "max_lr = 5e-5\n",
    "# how many pretrained layers to freeze\n",
    "freeze_layers = 0\n",
    "# number gpus\n",
    "num_gpus = 1\n",
    "# number cpu cores\n",
    "num_proc = 16\n",
    "# batch size for training and eval\n",
    "geneformer_batch_size = 12\n",
    "# learning schedule\n",
    "lr_schedule_fn = \"linear\" #\"polynomial\", \"linear\", \"cosine\"\n",
    "# warmup steps\n",
    "warmup_steps = 500\n",
    "# number of epochs\n",
    "epochs = 20\n",
    "# optimizer\n",
    "optimizer = \"adamW\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05164c24-5fbf-4372-b26c-a43f3777a88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for organ in organ_list:\n",
    "    print(organ)\n",
    "    organ_trainset = trainset_dict[organ]\n",
    "    organ_evalset = evalset_dict[organ]\n",
    "    organ_label_dict = traintargetdict_dict[organ]\n",
    "    print(organ_label_dict)\n",
    "    \n",
    "    # set logging steps\n",
    "    logging_steps = round(len(organ_trainset)/geneformer_batch_size/10)\n",
    "\n",
    "    \n",
    "    pretrain_model = \"your mouse-Geneformer name\"\n",
    "\n",
    "    # reload pretrained model\n",
    "    model = BertForSequenceClassification.from_pretrained(\"/path/to/your/mouse-Geneformer/model/{}/models/\".format(pretrain_model), \n",
    "                                                            num_labels=len(organ_label_dict.keys()),\n",
    "                                                            output_attentions = False,\n",
    "                                                            output_hidden_states = False).to(\"cuda\")\n",
    "\n",
    "    # define output directory path\n",
    "    current_date = datetime.datetime.now()\n",
    "    datestamp = f\"{str(current_date.year)[-2:]}{current_date.month:02d}{current_date.day:02d}\"\n",
    "    if f_type == \"isp\" :\n",
    "        output_dir = f\"/path/to/your/fine-tuning/model/to/save/in_silico_pretraining/{datestamp}_mouse-geneformer_CellClassifier_{organ}_L{max_input_size}_B{geneformer_batch_size}_LR{max_lr}_LS{lr_schedule_fn}_WU{warmup_steps}_E{epochs}_O{optimizer}_F{freeze_layers}_ISP-{organ}/\"\n",
    "    elif f_type == \"ctc\" :\n",
    "        output_dir = f\"/path/to/your/fine-tuning/model/to/save/cell_type_classification/{datestamp}_mouse-geneformer_DiseaseClassifier_{organ}_L{max_input_size}_B{geneformer_batch_size}_LR{max_lr}_LS{lr_schedule_fn}_WU{warmup_steps}_E{epochs}_O{optimizer}_F{freeze_layers}_CTC-{organ}/\"\n",
    "    else :\n",
    "        print(\"error: select fine turining type (ctc or isp)\")\n",
    "        sys.exit(1)\n",
    "    # ensure not overwriting previously saved model\n",
    "    saved_model_test = os.path.join(output_dir, f\"pytorch_model.bin\")\n",
    "    if os.path.isfile(saved_model_test) == True:\n",
    "        raise Exception(\"Model already saved to this directory.\")\n",
    "        \n",
    "    # make output directory\n",
    "    subprocess.call(f'mkdir {output_dir}', shell=True)\n",
    "\n",
    "    # set training arguments\n",
    "    training_args = {\n",
    "        \"learning_rate\": max_lr,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"logging_steps\": logging_steps,\n",
    "        \"group_by_length\": True,\n",
    "        \"length_column_name\": \"length\",\n",
    "        \"disable_tqdm\": False,\n",
    "        \"lr_scheduler_type\": lr_schedule_fn,\n",
    "        \"warmup_steps\": warmup_steps,\n",
    "        \"weight_decay\": 0.001,\n",
    "        \"per_device_train_batch_size\": geneformer_batch_size,\n",
    "        \"per_device_eval_batch_size\": geneformer_batch_size,\n",
    "        \"num_train_epochs\": epochs,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"output_dir\": output_dir,\n",
    "        #\"max_position_embeddings\": 2**11,\n",
    "    }\n",
    "    \n",
    "    training_args_init = TrainingArguments(**training_args)\n",
    "\n",
    "    # create the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args_init,\n",
    "        data_collator=DataCollatorForCellClassification(),\n",
    "        train_dataset=organ_trainset,\n",
    "        eval_dataset=organ_evalset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    # train the cell type classifier\n",
    "    trainer.train()\n",
    "    predictions = trainer.predict(organ_evalset)\n",
    "    with open(f\"{output_dir}predictions.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(predictions, fp)\n",
    "    trainer.save_metrics(\"eval\",predictions.metrics)\n",
    "    trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f15f51c-7908-4823-aa63-05142dae1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove cache files in xxx.dataset\n",
    "\n",
    "rmfiles = glob.glob(dataset_name+\"/cache*\")\n",
    "#print(rmfiles)\n",
    "for tqdm_i2, rmfile in zip(tqdm(rmfiles, desc='remove files loop'), rmfiles) :\n",
    "    os.remove(rmfile)\n",
    "print(\"Finished removing cache files in this dataset!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b1bb04-5f59-4071-ac7e-267b96501d96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "eba1599a1f7e611c14c87ccff6793920aa63510b01fc0e229d6dd014149b8829"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
