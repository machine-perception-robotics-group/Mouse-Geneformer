{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8f83ba1-d75c-44bb-b508-bef0048e5dfd",
   "metadata": {},
   "source": [
    "# Geneformer zero-shot cell type classification with knn classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e939b2-4975-4be1-9144-6503f87ad0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GPU_NUMBER = [0] # set cuda number to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(s) for s in GPU_NUMBER])\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b8e8f5-b800-422d-99f7-93445bbc0aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import pickle\n",
    "import subprocess\n",
    "import seaborn as sns; sns.set()\n",
    "from datasets import load_from_disk\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import BertConfig, BertForSequenceClassification, BertForMaskedLM, BertModel\n",
    "from transformers import Trainer\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "from geneformer import DataCollatorForCellClassification\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8cf2f-a89b-4d5d-b79f-851161fe6c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cell type dataset (includes all tissues)\n",
    "\n",
    "\n",
    "# select task type (ctc: cell type classification, isp: in silico perturbation)\n",
    "f_type = ctc\n",
    "\n",
    "# dataset_name (xxx.dataset path) \n",
    "dataset_name = \"/path/to/your/dataset/to/analysis/xxx.dataset\"\n",
    "\n",
    "\n",
    "train_dataset=load_from_disk(dataset_name)\n",
    "\n",
    "# check and remove column names\n",
    "if f_type == \"isp\" :\n",
    "    try :\n",
    "        print(np.unique(train_dataset[\"disease\"]))\n",
    "    except KeyError as e :\n",
    "        print(\"KeyError: {}\".format(e))\n",
    "        print(\"changing to disease\")\n",
    "        train_dataset = train_dataset.rename_column(\"column name in diseases infomation\",\"disease\")\n",
    "        print(\"change finished\")\n",
    "        print(np.unique(train_dataset[\"disease\"]))\n",
    "    \n",
    "elif f_type == \"ctc\" :\n",
    "    try :\n",
    "        print(np.unique(train_dataset[\"cell_type\"]))\n",
    "    except KeyError as e :\n",
    "        print(\"KeyError: {}\".format(e))\n",
    "        print(\"changing to cell_type\")\n",
    "        train_dataset = train_dataset.rename_column(\"column name in cell types infomation\",\"cell_type\")\n",
    "        print(\"change finished\")\n",
    "        print(np.unique(train_dataset[\"cell_type\"]))\n",
    "\n",
    "else :\n",
    "    print(\"error: select fine turning type (ctc or isp)\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feae6c1e-a6a0-405c-833b-3aa19ed6fc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "rmfiles = glob.glob(dataset_name+\"/cache*\")\n",
    "#print(rmfiles)\n",
    "if rmfiles == [] :\n",
    "    print(\"not exist cache files\")\n",
    "else :\n",
    "    for tqdm_i2, rmfile in zip(tqdm(rmfiles, desc='remove files loop'), rmfiles) :\n",
    "        os.remove(rmfile)\n",
    "    print(\"removed cache files!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e29736-22f8-4af0-bf9c-ddeb74823376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "rmfiles = glob.glob(dataset_name+\"/tmp*\")\n",
    "#print(rmfiles)\n",
    "if rmfiles == [] :\n",
    "    print(\"not exist tmp... files\")\n",
    "else :\n",
    "    for tqdm_i2, rmfile in zip(tqdm(rmfiles, desc='remove files loop'), rmfiles) :\n",
    "        os.remove(rmfile)\n",
    "    print(\"removed tmp... files!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75656d-2191-4558-982f-3db64a72fe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset_list = []\n",
    "evalset_list = []\n",
    "organ_list = []\n",
    "target_dict_list = []\n",
    "\n",
    "\n",
    "for organ in Counter(train_dataset[\"organ_major\"]).keys():\n",
    "    # collect list of tissues for fine-tuning (immune and bone marrow are included together)\n",
    "    if organ in [\"bone_marrow\"]:  \n",
    "        continue\n",
    "    elif organ==\"immune\":\n",
    "        organ_ids = [\"immune\",\"bone_marrow\"]\n",
    "        organ_list += [\"immune\"]\n",
    "    else:\n",
    "        organ_ids = [organ]\n",
    "        organ_list += [organ]\n",
    "    \n",
    "    print(organ)\n",
    "    \n",
    "    # filter datasets for given organ\n",
    "    def if_organ(example):\n",
    "        return example[\"organ_major\"] in organ_ids\n",
    "    trainset_organ = train_dataset.filter(if_organ, num_proc=16)\n",
    "    \n",
    "    # per scDeepsort published method, drop cell types representing <0.5% of cells\n",
    "    celltype_counter = Counter(trainset_organ[\"cell_type\"])\n",
    "    total_cells = sum(celltype_counter.values())\n",
    "    cells_to_keep = [k for k,v in celltype_counter.items() if v>(0.005*total_cells)]\n",
    "    def if_not_rare_celltype(example):\n",
    "        return example[\"cell_type\"] in cells_to_keep\n",
    "    trainset_organ_subset = trainset_organ.filter(if_not_rare_celltype, num_proc=16)\n",
    "      \n",
    "    # shuffle datasets and rename columns\n",
    "    trainset_organ_shuffled = trainset_organ_subset.shuffle(seed=42)\n",
    "    trainset_organ_shuffled = trainset_organ_shuffled.rename_column(\"cell_type\",\"label\")\n",
    "    trainset_organ_shuffled = trainset_organ_shuffled.remove_columns(\"organ_major\")\n",
    "    \n",
    "    # create dictionary of cell types : label ids\n",
    "    target_names = list(Counter(trainset_organ_shuffled[\"label\"]).keys())\n",
    "    target_name_id_dict = dict(zip(target_names,[i for i in range(len(target_names))]))\n",
    "    target_dict_list += [target_name_id_dict]\n",
    "    \n",
    "    # change labels to numerical ids\n",
    "    def classes_to_ids(example):\n",
    "        example[\"label\"] = target_name_id_dict[example[\"label\"]]\n",
    "        return example\n",
    "    labeled_trainset = trainset_organ_shuffled.map(classes_to_ids, num_proc=16)\n",
    "    \n",
    "    # create 80/20 train/eval splits\n",
    "    labeled_train_split = labeled_trainset.select([i for i in range(0,round(len(labeled_trainset)*0.8))])\n",
    "    labeled_eval_split = labeled_trainset.select([i for i in range(round(len(labeled_trainset)*0.8),len(labeled_trainset))])\n",
    "    \n",
    "    # filter dataset for cell types in corresponding training set\n",
    "    trained_labels = list(Counter(labeled_train_split[\"label\"]).keys())\n",
    "    def if_trained_label(example):\n",
    "        return example[\"label\"] in trained_labels\n",
    "    labeled_eval_split_subset = labeled_eval_split.filter(if_trained_label, num_proc=16)\n",
    "\n",
    "    dataset_list += [labeled_train_split]\n",
    "    evalset_list += [labeled_eval_split_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f1913f-80a8-4066-bdb0-5bc9a52963e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_dict = dict(zip(organ_list,dataset_list))\n",
    "traintargetdict_dict = dict(zip(organ_list,target_dict_list))\n",
    "\n",
    "evalset_dict = dict(zip(organ_list,evalset_list))\n",
    "\n",
    "\n",
    "print(trainset_dict)\n",
    "print(traintargetdict_dict)\n",
    "\n",
    "print(evalset_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3773c984-2e2f-4be0-8e7c-35d9495434f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds, labels):\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    pre = precision_score(labels, preds, average='macro')\n",
    "    rec = recall_score(labels, preds, average='macro')\n",
    "    macro_f1 = f1_score(labels, preds, average='macro')\n",
    "    return {\n",
    "      'accuracy': acc,\n",
    "      'macro_precision': pre,\n",
    "      'macro_recall': rec,\n",
    "      'macro_f1': macro_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c70c3-c095-4cd4-ac9b-2efd36908ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model parameters\n",
    "# max input size\n",
    "max_input_size = 2**11  # 2048\n",
    "\n",
    "# set training hyperparameters\n",
    "# max learning rate\n",
    "max_lr = 5e-5\n",
    "# how many pretrained layers to freeze\n",
    "freeze_layers = 0\n",
    "# number gpus\n",
    "num_gpus = 1\n",
    "# number cpu cores\n",
    "num_proc = 16\n",
    "# batch size for training and eval\n",
    "geneformer_batch_size = 12\n",
    "# learning schedule\n",
    "lr_schedule_fn = \"cosine\" #\"polynomial\", \"linear\", \"cosine\"\n",
    "# warmup steps\n",
    "warmup_steps = 500\n",
    "# number of epochs\n",
    "epochs = 10\n",
    "# optimizer\n",
    "optimizer = \"adamW\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf7f408-4bb4-40b8-aa26-065829de06c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for organ in organ_list:\n",
    "    print(\"=\"*50)\n",
    "    print(organ)\n",
    "    organ_trainset = trainset_dict[organ]\n",
    "    organ_evalset = evalset_dict[organ]\n",
    "    organ_label_dict = traintargetdict_dict[organ]\n",
    "    print(organ_label_dict)\n",
    "    \n",
    "    # set logging steps\n",
    "    logging_steps = round(len(organ_trainset)/geneformer_batch_size/10)\n",
    "\n",
    "\n",
    "    pretrain_model = \"your mouse-Geneformer name\"\n",
    "    \n",
    "    # reload pretrained model\n",
    "    model = BertModel.from_pretrained(\"/path/to/mouse-Geneformer/model/{}/models/\".format(pretrain_model), \n",
    "                                                            num_labels=len(organ_label_dict.keys()),\n",
    "                                                            output_attentions = False,\n",
    "                                                            output_hidden_states = False).to(\"cuda\")\n",
    "    \n",
    "\n",
    "    # define output directory path\n",
    "    current_date = datetime.datetime.now()\n",
    "    datestamp = f\"{str(current_date.year)[-2:]}{current_date.month:02d}{current_date.day:02d}\"\n",
    "    output_dir = f\"/path/to/your/zero-shot/directory/to/save/result/{datestamp}_mouse-geneformer_zero-shot-CellClassifier_{organ}_L{max_input_size}_B{geneformer_batch_size}_LR{max_lr}_LS{lr_schedule_fn}_WU{warmup_steps}_E{epochs}_O{optimizer}_F{freeze_layers}_CTC-{organ}/\"\n",
    "    # ensure not overwriting previously saved model\n",
    "            \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)   \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    train_pooled_embeddings = []\n",
    "    train_labels_list = []\n",
    "    \n",
    "    # get feature vector in each cell text in train data \n",
    "    for _, inputs, label in zip(tqdm(organ_trainset[\"input_ids\"], desc='input data loop'), organ_trainset[\"input_ids\"], organ_trainset[\"label\"]) :\n",
    "        inputs = torch.Tensor(inputs).to(torch.long).unsqueeze(0).to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "    \n",
    "        # save feature vector in list\n",
    "        pooled_emb_data = pooled_output.to(\"cpu\").detach().numpy().copy()\n",
    "        train_pooled_embeddings.append(pooled_emb_data)\n",
    "\n",
    "        # save label of feature vector in list        \n",
    "        train_labels_list.append(label)\n",
    "\n",
    "    train_data_feature_array = np.array(train_pooled_embeddings)\n",
    "    train_features = train_data_feature_array.reshape(-1, train_data_feature_array.shape[2])\n",
    "\n",
    "    train_labels = np.array(train_labels_list)\n",
    "\n",
    "    del train_pooled_embeddings, train_labels_list\n",
    "\n",
    "    \n",
    "\n",
    "    test_pooled_embeddings = []\n",
    "    test_labels_list = []\n",
    "    \n",
    "    # get feature vector in each cell text in test data\n",
    "    for _, inputs, label in zip(tqdm(organ_evalset[\"input_ids\"], desc='input data loop'), organ_evalset[\"input_ids\"], organ_evalset[\"label\"]) :\n",
    "        inputs = torch.Tensor(inputs).to(torch.long).unsqueeze(0).to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        sequence_output, pooled_output = outputs[:2]\n",
    "    \n",
    "        # save feature vector in list\n",
    "        pooled_emb_data = pooled_output.to(\"cpu\").detach().numpy().copy()\n",
    "        test_pooled_embeddings.append(pooled_emb_data)\n",
    "\n",
    "        # save label of feature vector in list        \n",
    "        test_labels_list.append(label)\n",
    "        \n",
    "    test_data_feature_array = np.array(test_pooled_embeddings)\n",
    "    test_features = test_data_feature_array.reshape(-1, test_data_feature_array.shape[2])\n",
    "\n",
    "    test_labels = np.array(test_labels_list)\n",
    "\n",
    "    del test_pooled_embeddings, test_labels_list\n",
    "\n",
    "\n",
    "    # cell classification using k-nearest neighbor methods\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric='cosine')\n",
    "    knn.fit(train_features, train_labels)\n",
    "\n",
    "    test_pred_labels = knn.predict(test_features)\n",
    "\n",
    "    result_dict = compute_metrics(test_pred_labels, test_labels)\n",
    "\n",
    "    \n",
    "    print(\"accuracy: {}, precision: {}, recall: {}, f1_score: {}\".format(result_dict[\"accuracy\"], result_dict[\"macro_precision\"], result_dict[\"macro_recall\"], result_dict[\"macro_f1\"]))\n",
    "\n",
    "\n",
    "    with open(output_dir+organ+\"_zero-shot-knn_result.json\", \"w\") as f :\n",
    "        json.dump(result_dict, f)\n",
    "    print(\"saved zero-shot classification result!\")\n",
    "    \n",
    "print(\"finish\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c108065-1e5d-4655-a041-fa615ca198aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rmfiles = glob.glob(dataset_name+\"/cache*\")\n",
    "#print(rmfiles)\n",
    "for tqdm_i2, rmfile in zip(tqdm(rmfiles, desc='remove files loop'), rmfiles) :\n",
    "    os.remove(rmfile)\n",
    "print(\"removed cache files!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aee530-4012-4698-a217-92a4c614db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "zs_reslut_dicts = glob.glob(\"/path/to/your/zero-shot/directory/to/save/result/*\")\n",
    "print(len(zs_reslut_dicts))\n",
    "\n",
    "organ = [\"set\", \"each\", \"organ\", \"name\", \"to\", \"watch\", \"zero-shot\", \"classification\", \"result\"]\n",
    "for zs_result_dict in zs_reslut_dicts :\n",
    "    print(\"=\"*100)\n",
    "    json_path = glob.glob(zs_result_dict+\"/*.json\")[0]\n",
    "    match = re.search(r'CellClassifier[.[A-Z]*.]*(?P<organ>.*?)_L2048', json_path)\n",
    "    print(match.group('organ'))\n",
    "    json_open = open(json_path, 'r')\n",
    "    result = json.load(json_open)\n",
    "    print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a847bd96-71c6-468c-804c-cd8821538ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff72840-21bb-41dd-9755-69613579547c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2d4e85-1927-40c3-bd8e-e27ebfa5ae40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "eba1599a1f7e611c14c87ccff6793920aa63510b01fc0e229d6dd014149b8829"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
